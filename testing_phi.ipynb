{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import peft \n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from utils.dataset import template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = 'GeneZC/MiniChat-3B'\n",
    "model_id = 'microsoft/phi-2'\n",
    "model_type = 'phi'\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6972098ca04afabfb050bdf420e34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    flash_attn=True,\n",
    "    flash_rotary=True,\n",
    "    fused_dense=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map={'': 0},\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_path = 'lora/disco-limbic-dialogue-512/'\n",
    "lora_path = 'lora/disco-limbic-dialogue-phi2-eos/'\n",
    "\n",
    "model = model.eval()\n",
    "lora_model = peft.PeftModel.from_pretrained(model, lora_path, adapter_name='loraTrained', is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xx/miniconda3/envs/disco/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are the parts of the human brain that conduct a dialogue, you can enter into verbal altercations with the interlocutor. You need to response emotionally.\n",
      "[|Assistant|] [Electrochemistry]: Whoa! In your hand: *pyrholidon* -- the double rainbow of synthetic hallucinogens. Rare and gritty, a product of the age of atomic power.<|endoftext|>\n",
      "[|User|] Look at the little puck of liquid.<|endoftext|>\n",
      "[|Assistant|] [Electrochemistry]: What a funny little cap! Don't let the *scary* medical warnings throw you off. It's an inadequate antidote to radiation poisoning, but a *potent* antidote to *boredom*.<|endoftext|>\n",
      "[|User|] Hmm... open the cap.<|endoftext|>\n",
      " [|Assistant|] [Electrochemistry]: There it is again, like clockwork, doing its bidding in the dark recesses of your nervous system... But don't trust this drug company spin any more than you have already. This stuff has some serious side effects.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "dialog = [\n",
    "    \"[Electrochemistry]: Whoa! In your hand: *pyrholidon* -- the double rainbow of synthetic hallucinogens. Rare and gritty, a product of the age of atomic power.\",\n",
    "    \"Look at the little puck of liquid.\",\n",
    "    \"[Electrochemistry]: What a funny little cap! Don't let the *scary* medical warnings throw you off. It's an inadequate antidote to radiation poisoning, but a *potent* antidote to *boredom*.\",\n",
    "    \"Hmm... open the cap.\",\n",
    "    #'Look around',\n",
    "    #'\"How can I take shit without taking off my sweater?\"',\n",
    "    #'who am i?</s>'\n",
    "]\n",
    "\n",
    "query = template(dialog, model_type) + ' [|Assistant|] '\n",
    "model_inputs = tokenizer(query, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "generated_ids = model.generate(input_ids=model_inputs, max_new_tokens=64,\n",
    "                               do_sample=True,\n",
    "                               #pad_token_id=tokenizer.pad_token_id,\n",
    "                               temperature=0.7,\n",
    "                               repetition_penalty=1.15)\n",
    "#outputs = model(input_ids=input_ids),# max_length=cut_len, min_length=8, top_p=0.9, do_sample=True)\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/set']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/set'.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "то есть доведение до суицида да\n"
     ]
    }
   ],
   "source": [
    "print('\\u0442\\u043e \\u0435\\u0441\\u0442\\u044c \\u0434\\u043e\\u0432\\u0435\\u0434\\u0435\\u043d\\u0438\\u0435 \\u0434\\u043e \\u0441\\u0443\\u0438\\u0446\\u0438\\u0434\\u0430 \\u0434\\u0430')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[You]: how are you?\n",
      "\n",
      "[Pain Threshold]:  Hell. What hell is this?!<|endoftext|>\n",
      "\n",
      "[You]: what do you mean?\n",
      "\n",
      "[Pain Threshold]:  HELL. This little pain... it's *unbearable*. I'm done here!\n",
      "[Half Light]: Just keep breathing -- go easy on yourself. No one likes being in pain, but at least there are people who care about you. That feels better already.<|endoftext|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dialog = [\n",
    "]\n",
    "\n",
    "answer_start = '[Pain Threshold]: '\n",
    "\n",
    "while True:\n",
    "    inp = input('Input:')\n",
    "    if inp == 'q':\n",
    "        break\n",
    "    if inp:\n",
    "        dialog.append(inp)\n",
    "        print(f'[You]: {inp}', end='\\n\\n')\n",
    "\n",
    "    query = template(dialog, model_type) + ' [|Assistant|] ' + answer_start\n",
    "    \n",
    "    model_inputs = tokenizer(query, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "    input_len = len(model_inputs[0])\n",
    "    generated_ids = model.generate(input_ids=model_inputs, max_new_tokens=512,\n",
    "                                do_sample=True,\n",
    "                                #pad_token_id=tokenizer.eos_token_id,\n",
    "                                temperature=0.7,\n",
    "                                repetition_penalty=1.15)\n",
    "    output = answer_start + tokenizer.decode(generated_ids[0][input_len:],\n",
    "                                             skip_special_tokens=False)\n",
    "    dialog.append(output)\n",
    "    print(output, end='\\n\\n')\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = template(['/'], model_type) + ' [|Assistant|] ' + answer_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Pain Threshold]: <|endoftext|>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are the parts of the human brain that conduct a dialogue, you can enter into verbal altercations with the interlocutor. You need to response emotionally.\\n[|User|] /<|endoftext|>\\n [|Assistant|] [Pain Threshold]: '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = output.replace('[|Assistant|] ', '<<BREAK>>').replace('[|User|] ', '<<BREAK>>').split('<<BREAK>>')\n",
    "for i in a[1:]:\n",
    "    line = i.strip()\n",
    "    if line[0] != '[':\n",
    "        print(f'[You]: {line}', end='\\n\\n')\n",
    "    else:\n",
    "        print(f'{line}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asdf'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_conv(output):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
