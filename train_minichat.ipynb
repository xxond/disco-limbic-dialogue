{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xx/miniconda3/envs/disco/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xx/miniconda3/envs/disco/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_int8_training\n",
    "from contextlib import nullcontext\n",
    "from transformers import (default_data_collator, Trainer, TrainingArguments,\n",
    "                          TrainerCallback, AutoTokenizer, AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig)\n",
    "\n",
    "from transformers.integrations import WandbCallback\n",
    "\n",
    "from utils.dataset import CombineDataset, template\n",
    "\n",
    "# setting up wandb\n",
    "%env WANDB_PROJECT=disco-limbic-dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model_id = 'GeneZC/MiniChat-3B'\n",
    "\n",
    "# data settings\n",
    "dataset_train_path = 'data/dataset/v1/train.json'\n",
    "dataset_test_path = 'data/dataset/v1/test.json'\n",
    "max_data_length = 256*2\n",
    "\n",
    "# lora settings\n",
    "lora_r = 32\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    \n",
    "\n",
    "# train settings \n",
    "device = 'cuda'\n",
    "lr = 3e-4\n",
    "num_train_epochs = 5\n",
    "\n",
    "gradient_accumulation_steps = 16\n",
    "per_device_train_bs = 2\n",
    "per_device_eval_bs = 4\n",
    "\n",
    "log_steps = 10\n",
    "eval_steps = 30\n",
    "\n",
    "\n",
    "output_dir = f'lora/disco-limbic-dialogue-phi2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, use_cache=True,\n",
    "                                             device_map=device,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4878 1344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"data_test = []\\nfor i in raw_data_test:\\n    if len(tokenizer(template(i))['input_ids']) < max_data_length:\\n        data_test.append(i)\\nprint(len(raw_data_test), len(data_test))\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(dataset_train_path, 'r') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "with open(dataset_test_path, 'r') as f:\n",
    "    raw_data_test = json.load(f)\n",
    "\n",
    "## filter\n",
    "data_train = []\n",
    "for i in raw_data:\n",
    "    if len(tokenizer(template(i))['input_ids']) < max_data_length:\n",
    "        data_train.append(i)\n",
    "print(len(raw_data), len(data_train))\n",
    "\n",
    "data_test = []\n",
    "for i in raw_data_test:\n",
    "    if len(tokenizer(template(i))['input_ids']) < max_data_length:\n",
    "        data_test.append(i)\n",
    "print(len(raw_data_test), len(data_test))\n",
    "\n",
    "\n",
    "train_dataset = CombineDataset(data_train, tokenizer, max_words=max_data_length)\n",
    "test_dataset = CombineDataset(data_test, tokenizer, max_words=max_data_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 3,020,743,680 || trainable%: 0.009762893884462253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xx/miniconda3/envs/disco/lib/python3.10/site-packages/peft/utils/other.py:136: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "def create_peft_config(model):\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        target_modules = target_modules\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_profiler = False\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': lr,\n",
    "    'num_train_epochs': num_train_epochs,\n",
    "    'gradient_accumulation_steps': gradient_accumulation_steps,\n",
    "    'per_device_train_batch_size': per_device_train_bs,\n",
    "    'gradient_checkpointing': False,\n",
    "}\n",
    "\n",
    "profiler = nullcontext()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(tokenizer, predictions):\n",
    "    prediction_text = tokenizer.batch_decode(predictions.predictions.argmax(axis=-1))\n",
    "    return {\"predictions\": prediction_text} # \"labels\": labels, \n",
    "\n",
    "\n",
    "class WandbPredictionProgressCallback(WandbCallback):\n",
    "    def __init__(self, trainer, tokenizer, val_dataset, num_samples=100, freq=2):\n",
    "        super().__init__()\n",
    "        self.trainer = trainer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_dataset = [val_dataset[i] for i in range(num_samples)]\n",
    "        self.freq = freq\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        super().on_evaluate(args, state, control, **kwargs)\n",
    "        if state.global_step % self.freq == 0:\n",
    "            predictions = self.trainer.predict(self.sample_dataset)\n",
    "            predictions = decode_predictions(self.tokenizer, predictions)\n",
    "            predictions_df = pd.DataFrame(predictions)\n",
    "            predictions_df[\"epoch\"] = state.epoch\n",
    "            records_table = self._wandb.Table(dataframe=predictions_df)\n",
    "            self._wandb.log({\"sample_predictions\": records_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_dir)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    #bf16=True,  # Use BF16 if available\n",
    "    ## eval strat\n",
    "    do_eval=True,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=eval_steps,\n",
    "    per_device_eval_batch_size=per_device_eval_bs,\n",
    "    ## logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=log_steps,\n",
    "    ## wandb\n",
    "    report_to=\"wandb\",\n",
    "    run_name=output_dir.split('/')[-1],\n",
    "    ## other\n",
    "    save_strategy=\"no\",\n",
    "    #optim=\"adamw_torch_fused\",\n",
    "    max_steps=-1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    # Create Trainer instance``\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        callbacks=[],\n",
    "    )\n",
    "    # Instantiate the WandbPredictionProgressCallback\n",
    "    progress_callback = WandbPredictionProgressCallback(\n",
    "        trainer=trainer,\n",
    "        tokenizer=tokenizer,\n",
    "        val_dataset=test_dataset,\n",
    "        num_samples=16,\n",
    "        freq=30,\n",
    "    )\n",
    "\n",
    "    # Add the callback to the trainer\n",
    "    trainer.add_callback(progress_callback)\n",
    "    \n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
